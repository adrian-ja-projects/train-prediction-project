{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pl_transf_staging_to_raw",
      "provenance": [],
      "authorship_tag": "ABX9TyPNhc6VtT6z1TSOr+4QOaYU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adrian-ja-projects/train-prediction-project/blob/fea_data_analisys/pl_transf_staging_to_raw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3yUPSOToNy4"
      },
      "source": [
        "## Pipeline to write into RAw file ready for db use\n",
        "1. The pipeline first flatten the json file\n",
        "2. Loop through the files in the staging\n",
        "3. Create the parquet table or append the data into the table. \n",
        "4. It provides feedback in case the json file is empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-AM3F0GQaku"
      },
      "source": [
        "## Installing dependencies for the pipeline to work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGRyGjQFn1pm"
      },
      "source": [
        "from pyspark.sql.functions import col, explode_outer\n",
        "from pyspark.sql.types import *\n",
        "from copy import deepcopy\n",
        "from collections import Counter\n",
        "from pyspark.sql import DataFrame as SparkDataFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIJD9_8pnIj1"
      },
      "source": [
        "%run /content/train-prediction-project/AutoFlatten_py.ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LJEBevQQvr1"
      },
      "source": [
        "##  Create spark app session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPz538SbQ0l1"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"pl_transf_staging_to_raw\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynCAlYtsjn6P"
      },
      "source": [
        "def auto_flatting_json(file_path: str)-> SparkDataFrame:\n",
        "  \"\"\"\n",
        "  method to flatten the json files programmatically\n",
        "  \"\"\"\n",
        "\n",
        "  json_df = spark.read.format(\"json\").options(multiline=\"True\").load(file_path)\n",
        "  json_schema = json_df.schema\n",
        "\n",
        "  af = AutoFlatten(json_schema)\n",
        "  af.compute()\n",
        "\n",
        "  df1 = json_df\n",
        "  \n",
        "  ### Source of the code: https://towardsdatascience.com/flattening-json-records-using-pyspark-b83137669def\n",
        "  visited = set([f'.{column}' for column in df1.columns])\n",
        "  duplicate_target_counter = Counter(af.all_fields.values())\n",
        "  cols_to_select = df1.columns\n",
        "  for rest_col in af.rest:\n",
        "      if rest_col not in visited:\n",
        "          cols_to_select += [rest_col[1:]] if (duplicate_target_counter[af.all_fields[rest_col]]==1 and af.all_fields[rest_col] not in df1.columns) else [col(rest_col[1:]).alias(f\"{rest_col[1:].replace('.', '>')}\")]\n",
        "          visited.add(rest_col)\n",
        "\n",
        "  df1 = df1.select(cols_to_select)\n",
        "\n",
        "  \n",
        "  if af.order:\n",
        "      for key in af.order:\n",
        "          column = key.split('.')[-1]\n",
        "          if af.bottom_to_top[key]:\n",
        "              #########\n",
        "              #values for the column in bottom_to_top dict exists if it is an array type\n",
        "              #########\n",
        "              df1 = df1.select('*', explode_outer(col(column)).alias(f\"{column}_exploded\")).drop(column)\n",
        "              data_type = df1.select(f\"{column}_exploded\").schema.fields[0].dataType\n",
        "              if not (isinstance(data_type, StructType) or isinstance(data_type, ArrayType)):\n",
        "                  df1 = df1.withColumnRenamed(f\"{column}_exploded\", column if duplicate_target_counter[af.all_fields[key]]<=1 else key[1:].replace('.', '>'))\n",
        "                  visited.add(key)\n",
        "              else:\n",
        "                  #grabbing all paths to columns after explode\n",
        "                  cols_in_array_col = set(map(lambda x: f'{key}.{x}', df1.select(f'{column}_exploded.*').columns))\n",
        "                  #retrieving unvisited columns\n",
        "                  cols_to_select_set = cols_in_array_col.difference(visited)\n",
        "                  all_cols_to_select_set = set(af.bottom_to_top[key])\n",
        "                  #check done for duplicate column name & path\n",
        "                  cols_to_select_list = list(map(lambda x: f\"{column}_exploded{'.'.join(x.split(key)[1:])}\" if (duplicate_target_counter[af.all_fields[x]]<=1 and x.split('.')[-1] not in df1.columns) else col(f\"{column}_exploded{'.'.join(x.split(key)[1:])}\").alias(f\"{x[1:].replace('.', '>')}\"), list(all_cols_to_select_set)))\n",
        "                  #updating visited set\n",
        "                  visited.update(cols_to_select_set)\n",
        "                  rem = list(map(lambda x: f\"{column}_exploded{'.'.join(x.split(key)[1:])}\", list(cols_to_select_set.difference(all_cols_to_select_set))))\n",
        "                  df1 = df1.select(df1.columns + cols_to_select_list + rem).drop(f\"{column}_exploded\")        \n",
        "          else:\n",
        "              #########\n",
        "              #values for the column in bottom_to_top dict do not exist if it is a struct type / array type containing a string type\n",
        "              #########\n",
        "              #grabbing all paths to columns after opening\n",
        "              cols_in_array_col = set(map(lambda x: f'{key}.{x}', df1.selectExpr(f'{column}.*').columns))\n",
        "              #retrieving unvisited columns\n",
        "              cols_to_select_set = cols_in_array_col.difference(visited)\n",
        "              #check done for duplicate column name & path\n",
        "              cols_to_select_list = list(map(lambda x: f\"{column}.{x.split('.')[-1]}\" if (duplicate_target_counter[x.split('.')[-1]]<=1 and x.split('.')[-1] not in df1.columns) else col(f\"{column}.{x.split('.')[-1]}\").alias(f\"{x[1:].replace('.', '>')}\"), list(cols_to_select_set)))\n",
        "              #updating visited set\n",
        "              visited.update(cols_to_select_set)\n",
        "              df1 = df1.select(df1.columns + cols_to_select_list).drop(f\"{column}\")\n",
        "\n",
        "  \n",
        "  return df1.select([field[1:].replace('.', '>') if duplicate_target_counter[af.all_fields[field]]>1 else af.all_fields[field] for field in af.all_fields])\n",
        "\n",
        "def _check_object_exists(file_path: str)-> bool:\n",
        "  \"\"\"\n",
        "  method to check if object exists\n",
        "  \"\"\"\n",
        "  return exists(file_path)\n",
        "\n",
        "def write_data_into_table(table_name: str, parquet_table_location: str, inputDF: SparkDataFrame, write_mode: str, origin_file_path: str)-> None:\n",
        "  \"\"\"\n",
        "  check if object exist if it doesn't create a parquet table\n",
        "  \"\"\"\n",
        "  if not  _check_object_exists:\n",
        "    inputDF.repartition(1).write.mode(write_mode).save(f\"/content/raw/digitraffic/27_schedule\")\n",
        "    print(\"Table created\")\n",
        "  else:\n",
        "    if write_mode == \"overwrite\":\n",
        "      inputDF.repartition(1).write.mode(write_mode).save(f\"/content/raw/digitraffic/27_schedule\")\n",
        "      #print(f\"INFO:Table {table_name} created or overwritten {origin_file_path}\")\n",
        "    elif write_mode == \"append\": \n",
        "      inputDF.repartition(1).write.mode(write_mode).save(f\"/content/raw/digitraffic/27_schedule\")\n",
        "      #print(f\"INFO: Table {table_name} appended new data {origin_file_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BctYSK8NbfEL"
      },
      "source": [
        "#Loop to get all json files in staging\n",
        "table_name = \"27_schedule\"\n",
        "parquet_table_location = \"/content/raw/digitraffic/27_schedule\"\n",
        "debug= False\n",
        "\n",
        "print(\"INFO: Starting transformation of json files...\")\n",
        "for folderYearMonth in os.listdir('/content/staging/digitraffic/stg_27_schedule'):\n",
        "  for folderYearMonthDay in os.listdir(f'/content/staging/digitraffic/stg_27_schedule/{folderYearMonth}/'):\n",
        "    for fileName in os.listdir(f'/content/staging/digitraffic/stg_27_schedule/{folderYearMonth}/{folderYearMonthDay}'):\n",
        "      file_path = f'/content/staging/digitraffic/stg_27_schedule/{folderYearMonth}/{folderYearMonthDay}/{fileName}'\n",
        "      \n",
        "      #Auto flatten json file for db\n",
        "      df_flat_json = auto_flatting_json(file_path)\n",
        "\n",
        "      #create and write table into parquet location\n",
        "      if df_flat_json.count() > 0:\n",
        "        ## only write when data frame is not empty\n",
        "        write_data_into_table(table_name, parquet_table_location, df_flat_json, \"append\", file_path)\n",
        "      else:\n",
        "        if debug:\n",
        "          print(f\"WARNING: {file_path} file doesn't contain data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEchmgrtwvM3"
      },
      "source": [
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MyPZumomsC2"
      },
      "source": [
        "print(\"Data in raw ready to upload to a db\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}