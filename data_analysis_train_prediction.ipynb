{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_analysis_train_prediction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMixVZTuyVXYBHjsvlzGx4A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adrian-ja-projects/train-prediction-project/blob/fea_data_analisys/data_analysis_train_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwZFZDZDsbl-",
        "outputId": "08f6d420-d44c-44c7-c462-c9e350a5d3f4"
      },
      "source": [
        "!git clone https://github.com/adrian-ja-projects/train-prediction-project.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'train-prediction-project'...\n",
            "remote: Enumerating objects: 59, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 59 (delta 39), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (59/59), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGt-oRBds_dK",
        "outputId": "571b5544-4c33-449c-b9f6-daf0ad85ebf9"
      },
      "source": [
        "%run /content/train-prediction-project/pl_extraction_api_to_staging.ipynb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Creating list for extraction loop...\n",
            "INFO: Extraction dates are between 2020-01-01 and 2020-04-09\n",
            "INFO: Starting extraction...\n",
            "INFO: Extraction completed a total of 100 file were successfully extracted into the staging area\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIJQeBfVtFJT",
        "outputId": "afd6b9a0-390e-4e95-d01e-74300ac63eea"
      },
      "source": [
        "%run /content/train-prediction-project/spark_dependencies.ipynb"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Installing Spark dependencies...\n",
            "INFO: Spark dependencies installed\n",
            "INFO: env variables created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sXbDu3kuVeI",
        "outputId": "ec4725c6-1b8c-4002-ea01-6a9d1db327cb"
      },
      "source": [
        "%run /content/train-prediction-project/pl_transf_staging_to_raw.ipynb"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Starting transformation of json files...\n",
            "Data in raw ready to upload to a db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfhBH9EQZU3L"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import TimestampType\n",
        "from pyspark.sql import Window"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNl6q5iQzdJd"
      },
      "source": [
        "#create spark session\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"pl_data_analysis\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oMnB-r30EEk"
      },
      "source": [
        "raw_file_path = \"/content/raw/digitraffic/27_schedule\"\n",
        "df_27_schedule = spark.read.format(\"parquet\").load(raw_file_path)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohL_qRGxY7dL",
        "outputId": "c98b248d-1888-443b-f96b-8c4c8e3a5e95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dateWindow = Window.partitionBy(\"departureDate\").orderBy(\"actualTime\")\n",
        "(df_27_schedule\n",
        " #First as best practice filter out data out of scope for the use case\n",
        " .select(\"departureDate\",\"stationShortCode\", \"actualTime\")\n",
        " .where(((F.col(\"stationShortCode\").isin([\"HKI\"]))&(F.col(\"type\")==\"DEPARTURE\"))\n",
        "        | ((F.col(\"stationShortCode\").isin([\"TPE\"]))&(F.col(\"type\")==\"ARRIVAL\")))\n",
        " #Transform data and get average travel duration time in minutes\n",
        " .withColumn(\"actualTime\", \n",
        "             F.col(\"actualTime\").cast(TimestampType()))\n",
        " .withColumn(\"travelDurationMinutes\", (F.unix_timestamp(F.col(\"actualTime\")) - F.unix_timestamp(F.lag(F.col(\"actualTime\"), 1)\n",
        "                                       .over(dateWindow)))/60)\n",
        " .where(F.col(\"stationShortCode\")==\"TPE\")\n",
        " .select(F.avg(\"travelDurationMinutes\").alias(\"avgTravelDurationMinutes\"))\n",
        ").show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------+\n",
            "|avgTravelDurationMinutes|\n",
            "+------------------------+\n",
            "|       94.62783333333334|\n",
            "+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PddcZ-rTpotN",
        "outputId": "d8b21c47-1c96-43c4-cfde-1409fe530ed9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        }
      },
      "source": [
        "(df_27_schedule\n",
        " #First as best practice filter out data out of scope for the use case\n",
        " .select(\"departureDate\",\"stationShortCode\", \"actualTime\")\n",
        " .where(((F.col(\"stationShortCode\").isin([\"TPE\"]))&(F.col(\"type\")==\"ARRIVAL\")))\n",
        " #Transform data and get average travel duration time in minutes\n",
        " .withColumn(\"actualTime\", \n",
        "             F.col(\"actualTime\").cast(TimestampType()))\n",
        " .groupBy(\"departureDate\").agg(F.stddev(\"actualTime\"))\n",
        " #.where(F.col(\"stationShortCode\")==\"TPE\")\n",
        " #.select(F.avg(\"travelDurationMinutes\").alias(\"avgTravelDurationMinutes\"))\n",
        ").show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-8ac9ad73ba78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m  .withColumn(\"actualTime\", \n\u001b[1;32m      7\u001b[0m              F.col(\"actualTime\").cast(TimestampType()))\n\u001b[0;32m----> 8\u001b[0;31m  \u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"departureDate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstddev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"actualTime\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m  \u001b[0;31m#.where(F.col(\"stationShortCode\")==\"TPE\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m  \u001b[0;31m#.select(F.avg(\"travelDurationMinutes\").alias(\"avgTravelDurationMinutes\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/group.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"all exprs should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             jdf = self._jgd.agg(exprs[0]._jc,\n\u001b[0;32m--> 119\u001b[0;31m                                 _to_seq(self.sql_ctx._sc, [c._jc for c in exprs[1:]]))\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.0-bin-hadoop3.2/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1310\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'stddev_samp(actualTime)' due to data type mismatch: argument 1 requires double type, however, 'actualTime' is of timestamp type.;\n'Aggregate [departureDate#45205], [departureDate#45205, stddev_samp(actualTime#47415) AS stddev_samp(actualTime)#47439]\n+- Project [departureDate#45205, stationShortCode#45224, cast(actualTime#45209 as timestamp) AS actualTime#47415]\n   +- Project [departureDate#45205, stationShortCode#45224, actualTime#45209]\n      +- Filter (stationShortCode#45224 IN (TPE) AND (type#45230 = ARRIVAL))\n         +- Project [departureDate#45205, stationShortCode#45224, actualTime#45209, type#45230]\n            +- Relation [cancelled#45203,commuterLineID#45204,departureDate#45205,operatorShortCode#45206,operatorUICCode#45207L,runningCurrently#45208,actualTime#45209,timeTableRows>cancelled#45210,categoryCode#45211,categoryCodeId#45212L,detailedCategoryCode#45213,detailedCategoryCodeId#45214L,thirdCategoryCode#45215,thirdCategoryCodeId#45216L,commercialStop#45217,commercialTrack#45218,countryCode#45219,differenceInMinutes#45220L,estimateSource#45221,liveEstimateTime#45222,scheduledTime#45223,stationShortCode#45224,stationUICCode#45225L,accepted#45226,... 10 more fields] parquet\n"
          ]
        }
      ]
    }
  ]
}